#!/usr/bin/env python
import sys
import os
import torch
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
import rospy
from std_msgs.msg import Float32
#  导入目录
sys.path.append('/home/zhang/pathplan/src/aapathplanning/environment')
from environment_sac_learn import Env
import numpy as np
import copy


#from sac_per import SAC,action_unnormalized
#from sac import SAC,action_unnormalized
#from sac_per_atten import SAC,action_unnormalized
from sac_atten import SAC, action_unnormalized

# ---Directory Path---#
# 获取绝对路径
dirPath = os.path.dirname(os.path.realpath(__file__))


if __name__ == '__main__':
    rospy.init_node('main')
    pub_result = rospy.Publisher('result', Float32, queue_size=5)
    result = Float32()# 获取消息类型
    success_count = 0  # 成功次数计数器

    action_dim = 2
    state_dim = 28
    max_steps = 600   #每一轮的步数
    num_episodes=100 #轮数
    past_action = np.array([0., 0.])  #获得浮点数组[0. 0.]
    results = []

    train_reward = np.empty((0, 3), np.float) # []
    test_reward = np.empty((0, 3), np.float)

    # env 无返回值
    env = Env(action_dim)

    # 无返回值
    agent = SAC(state_dim, action_dim)

    # 模型载入
    agent.load_models()

    for ep in range(num_episodes):
        # 返回激光数据的数组 和 初始化目标点
        state = env.reset() # (28,) 重置仿真环境
        #print("state{}".format(state.shape))
        rewards_current_episode = 0

        for step in range(max_steps):
            obstacle_min_range_list = []
            state = np.float32(state)
            ## shape(28,)
            
            if not ep % 10 == 0:
                # 选择随机动作
                action = agent.select_action(state)

            else:
                # 每10次选择mean动作 
                action = agent.select_action(state, eval=True)
                
            #(2,)   [0.9995601  0.38583654]

            # 获取动作
            unnorm_action = np.array([action_unnormalized(action[0], 0.3, 0.0),
                                      action_unnormalized(action[1], 2.0, -1.0)])

            # [9.03224945e-04 1.32221302e+00]

            next_state, reward, done, goalflag,obstacle_min_range= env.step(unnorm_action, past_action)
            # 状态、奖励、是否完成   以及 距离障碍物最小距离
            obstacle_min_range_list.append(obstacle_min_range)

            past_action = copy.deepcopy(action)

            if step ==max_steps-1:
                reward-=1000
                print('已超时')
            rewards_current_episode += reward

            next_state = np.float32(next_state)
            transition = (state, action, reward, next_state, done)

            agent.store_transition(transition)

            if agent.buffer_size() > 256:

                agent.update_parameters(agent.batch_size)
                
            state = copy.deepcopy(next_state)

        # 检查是否到达终点
            if done or goalflag:
                if goalflag:  # 成功到达目标
                    success_count += 1
                break  # 回合结束

        # 是否初始化目标点
            if goalflag:
                success_count += 1  # 每次成功就增加计数
                goalflag = False  # 重置标志，防止重复计数

        print("*****第{}轮结果*****".format(ep))
        # 统计最小碰撞距离
        min_obstacle_distance = min(obstacle_min_range_list) if obstacle_min_range_list else None
        print(f'Episode {ep} - Minimum Obstacle Distance: {min_obstacle_distance}')
        
        
        results.append(rewards_current_episode)

        print('Memory: ', agent.buffer_size())
        print('Episode: ', ep, 'reward per ep: ', rewards_current_episode)


        if (ep + 1) % num_episodes == 0:
            success_rate = (success_count / num_episodes) * 100  # 计算成功率（百分比）
            print(f'Success Rate after {ep + 1} episodes: {success_rate}%')
            success_count = 0  # 重置计数器，开始新的100轮统计
        
        train_reward = np.append(train_reward, np.asarray([[ep, rewards_current_episode, step]]), axis=0)
       
        np.savetxt(dirPath + '/reward/sac_reward.txt', train_reward)


        if ep > 10 and (ep + 1) % num_episodes == 0:
            print(f'Saving model at episode {ep + 1}')
            agent.save_models(ep + 1)